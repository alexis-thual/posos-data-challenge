{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Notebook\n",
    "This notebook is intended to present models which are loaded from other files as well as the results they allow us to reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import gensim\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CATEGORIES = 52\n",
    "PADDING = 150\n",
    "\n",
    "dataFolder = './../posos-data-challenge/challenge_data'\n",
    "xPath = os.path.join(dataFolder, 'input_train.csv')\n",
    "yPath = os.path.join(\n",
    "    dataFolder, 'challenge_output_data_training_file_predict_the_expected_answer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel():\n",
    "    '''Generic workflow class.'''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.verbose = kwargs.get('verbose', False)\n",
    "\n",
    "        self.nbCategories = kwargs['nbCategories']\n",
    "        self.paddingLength = PADDING\n",
    "        self.maxNumberWords = (1e5)\n",
    "        self.trainable = kwargs.get('trainable', False)\n",
    "\n",
    "        self.tokenizer = text.Tokenizer(num_words=self.maxNumberWords)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        '''Turns sentences into padded word sequences.'''\n",
    "\n",
    "        self.tokenizer.fit_on_texts(x)\n",
    "        sequences = self.tokenizer.texts_to_sequences(x)\n",
    "        sequences = sequence.pad_sequences(sequences, self.paddingLength)\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def preprocessLabels(self, labels):\n",
    "        return to_categorical(labels, num_classes=self.nbCategories)\n",
    "\n",
    "    def train(self, x, y, epochs= 10, batch_size=32, validation_data=None,\n",
    "              callback=False):\n",
    "        if callback == True:\n",
    "            filepath= 'models_checkpoints/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc',\n",
    "                                         verbose=1, save_best_only=True, mode='max')\n",
    "            callbacks_list = [checkpoint]\n",
    "\n",
    "            self.model.fit(x, y, shuffle='batch', epochs=epochs,\n",
    "                           batch_size=batch_size, validation_data=validation_data,\n",
    "                           callbacks=callbacks_list)\n",
    "        else:\n",
    "            self.model.fit(x, y, shuffle='batch', epochs=epochs,\n",
    "                           batch_size=batch_size, validation_data=validation_data)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        return self.model.evaluate(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize generic workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform spelling corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctionsPath = os.path.join(dataFolder, 'corrections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(CustomModel):\n",
    "    '''Extended CustomModel'''\n",
    "\n",
    "    def spellingCorrection(self, x, correct_dict={}, verbose=False):\n",
    "        corrected_x = []\n",
    "        for w in x.split():\n",
    "            if w in correct_dict.keys():\n",
    "                w_corrected = corrected_dict[w]\n",
    "                if verbose == True:\n",
    "                    print('Correction of ' + w + ' in ' + w_corrected)\n",
    "                w = w_corrected\n",
    "            corrected_x.append(w)\n",
    "        return ' '.join(corrected_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differenciate medics from other words\n",
    "We here use a list of medication names to distinguish better between common words and specialized ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste de médicaments regroupant les libéllés ATC et lesdénominations de spécialité, de taille: 8275\n",
      "Sample of medicament names:  ['ubistesin adrenalinee', 'forene,', 'sevorane,', 'chirocaïne', 'duodopa', 'synagis', 'kaletra', 'humira', 'norvir', 'viekirax']\n"
     ]
    }
   ],
   "source": [
    "MEDICAMENTS = []\n",
    "medicsPath = os.path.join(dataFolder, 'medicaments_france.xls')\n",
    "medic_db = pd.read_excel(medicsPath)\n",
    "\n",
    "for m in medic_db['Dénomination spécialité']:\n",
    "    med = []\n",
    "    for w in m.split():\n",
    "        if w.lower()!=w:\n",
    "            med.append(w)\n",
    "    med = ' '.join(med)\n",
    "    if len(med)!=0:\n",
    "        med = med.lower()\n",
    "        if med not in MEDICAMENTS:\n",
    "            MEDICAMENTS.append(med.lower())\n",
    "\n",
    "for m in medic_db['Libellé ATC']:\n",
    "    med = m.split()[0].lower()\n",
    "    if med not in MEDICAMENTS:\n",
    "        MEDICAMENTS.append(med)\n",
    "\n",
    "print('Liste de médicaments regroupant les libéllés ATC et les'\n",
    "      'dénominations de spécialité, de taille: {}'.format(len(MEDICAMENTS)))\n",
    "print('Sample of medicament names: ', MEDICAMENTS[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(CustomModel):\n",
    "    '''Extended CustomModel'''\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.medicaments = kwargs['medicaments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, GlobalMaxPooling1D\n",
    "\n",
    "class CustomModel(CustomModel):\n",
    "    '''Extended CustomModel'''\n",
    "\n",
    "    def buildCNN1D(self, embedding, drop_rate=0.3, nb_filters=128,\n",
    "                   filter_size=4, padding = PADDING):\n",
    "        self.embedding = embedding\n",
    "\n",
    "        my_input = keras.Input(shape=(self.paddingLength,), name= 'input')\n",
    "\n",
    "        embedding = (Embedding(input_dim=self.embedding.shape[0],\n",
    "                               output_dim=self.embedding.shape[1],\n",
    "                               weights=[self.embedding],\n",
    "                               input_length=self.paddingLength,\n",
    "                               trainable=self.trainable,\n",
    "                               name='embedding'))(my_input)\n",
    "        embedding_dropped = Dropout(drop_rate, name='drop0')(embedding)\n",
    "\n",
    "        conv1 = Conv1D(nb_filters, filter_size,\n",
    "                       activation='relu', name='conv1')(embedding_dropped)\n",
    "        pooled1 = GlobalMaxPooling1D(name='pool1')(conv1)\n",
    "        dropped1 = Dropout(drop_rate, name='drop1')(pooled1)\n",
    "        dense1 = Dense(self.nbCategories, activation = 'relu', name = 'dense1')(dropped1)\n",
    "        \n",
    "        prob = Dense(self.nbCategories, activation='softmax', name='softmax')(dense1)\n",
    "        \n",
    "        self.model = Model(my_input, prob)\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import LSTM, Embedding, Dropout\n",
    "\n",
    "\n",
    "class CustomModel(CustomModel):\n",
    "    '''Extended CustomModel'''\n",
    "    \n",
    "    def buildLSTM(self, embedding, drop_rate=0.3, nb_filters=128, filter_size=3):\n",
    "        self.embedding = embedding\n",
    "\n",
    "        my_input = Input(shape=(self.paddingLength,), name= 'input')\n",
    "\n",
    "        embedding = Embedding(input_length=self.paddingLength,\n",
    "                                input_dim=self.embedding.shape[0],\n",
    "                                output_dim=self.embedding.shape[1],\n",
    "                                weights=[self.embedding],\n",
    "                                trainable=self.trainable,\n",
    "                                name='embedding')(my_input)\n",
    "\n",
    "        embedding_dropped = Dropout(drop_rate, name='drop0')(embedding)\n",
    "#         CNN cell\n",
    "#         conv1 = Conv1D(nb_filters, filter_size, activation='relu', name='conv1',\n",
    "#                        padding = 'same')(embedding_dropped)\n",
    "#         pooled_conv1 = MaxPooling1D(pool_size = 2, name = 'pool1')(conv1)\n",
    "#         dropped1 = Dropout(drop_rate, name = 'drop1')(pooled_conv1)\n",
    "        lstm1 = LSTM(100, name = 'lstm1', dropout= drop_rate,\n",
    "                     recurrent_dropout= drop_rate)(embedding_dropped)\n",
    "        dense1 = Dense(self.nbCategories, activation = 'relu', name = 'dense1')(lstm1)\n",
    "        prob = Dense(self.nbCategories, activation='softmax', name='softmax')(dense1)\n",
    "\n",
    "        self.model = Model(my_input, prob)\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement a 2D convnet for text classification: \n",
    "#inspired from : https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import MaxPooling1D, LSTM, Conv1D\n",
    "\n",
    "\n",
    "class CustomModel(CustomModel):\n",
    "    '''Extended CustomModel'''\n",
    "    \n",
    "    def buildLSTM_CNN(self, embedding, drop_rate=0.3, nb_filters=128, filter_size=3):\n",
    "        self.embedding = embedding\n",
    "\n",
    "        my_input = Input(shape=(self.paddingLength,), name= 'input')\n",
    "\n",
    "        embedding = Embedding(input_length=self.paddingLength,\n",
    "                                input_dim=self.embedding.shape[0],\n",
    "                                output_dim=self.embedding.shape[1],\n",
    "                                weights=[self.embedding],\n",
    "                                trainable=self.trainable,\n",
    "                                name='embedding')(my_input)\n",
    "\n",
    "        embedding_dropped = Dropout(drop_rate, name='drop0')(embedding)\n",
    "        conv1 = Conv1D(nb_filters, filter_size, activation='relu',\n",
    "                       name='conv1', padding='same')(embedding_dropped)\n",
    "        pooled1 = MaxPooling1D(pool_size = 2, name = 'pool1')(conv1)\n",
    "        dropped1 = Dropout(drop_rate, name = 'drop1')(pooled1)\n",
    "        lstm1 = LSTM(100, name = 'lstm1',\n",
    "                     dropout= drop_rate, recurrent_dropout= drop_rate)(dropped1)\n",
    "        prob = Dense(self.nbCategories,\n",
    "                     activation='softmax', name='dense1')(lstm1)\n",
    "\n",
    "        self.model = Model(my_input, prob)\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement a 2D convnet for text classification: \n",
    "#inspiration is here : http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-b5765693e142>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-b5765693e142>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    class CustomModel(CustomModel):\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.layers import (Input, Conv2D, Dense, Dropout,\n",
    "                          MaxPooling2D, Flatten, Concatenate, Reshape)\n",
    "\n",
    "\n",
    "class CustomModel(CustomModel):\n",
    "    '''Extended CustomModel'''\n",
    "\n",
    "    def buildCNN2D(self, embedding, drop_rate=0.3, nb_filters=128, filter_size=[3, 5, 8],\n",
    "                   padding=PADDING):\n",
    "        if np.isscalar(filter_size):\n",
    "            filter_size = [3, 5, 8]\n",
    "            print(\"WARNING: You have to enter a list for the different\\\n",
    "            filter sizes, we modified directly to: {}\".format(filter_size))\n",
    "        \n",
    "        self.embedding = embedding\n",
    "\n",
    "        my_input = Input(shape=(self.paddingLength, ), name= 'input')\n",
    "\n",
    "        embedding = Embedding(input_dim=self.embedding.shape[0],\n",
    "                               output_dim=self.embedding.shape[1],\n",
    "                               weights=[self.embedding],\n",
    "                               input_length=self.paddingLength,\n",
    "                               trainable=self.trainable,\n",
    "                               name='embedding')(my_input)\n",
    "        embedding = Reshape((padding, self.embedding.shape[1], 1))(embedding)\n",
    "        embedding_dropped = Dropout(drop_rate, name='drop0')(embedding)\n",
    "        \n",
    "        # we concatenate 3 filter sizes\n",
    "        conv0 = Conv2D(nb_filters, (filter_size[0], self.embedding.shape[1]),\n",
    "                       activation='relu', name='conv0', padding='valid')(embedding_dropped)\n",
    "        pooled0 = MaxPooling2D(pool_size=(padding - filter_size[0] + 1, 1),\n",
    "                               strides=(1, 1), padding='valid', name='pool0')(conv0)\n",
    "        \n",
    "        conv1 = Conv2D(nb_filters, (filter_size[1], self.embedding.shape[1]),\n",
    "                       activation='relu', name='conv1', padding='valid')(embedding_dropped)\n",
    "        pooled1 = MaxPooling2D(pool_size = (padding - filter_size[1] + 1, 1),\n",
    "                               strides=(1, 1), padding='valid', name='pool1')(conv1)\n",
    "        \n",
    "        conv2 = Conv2D(nb_filters, (filter_size[2], self.embedding.shape[1]),\n",
    "                       activation='relu', name='conv2', padding='valid')(embedding_dropped)\n",
    "        pooled2 = MaxPooling2D(pool_size = (padding - filter_size[2] + 1, 1),\n",
    "                               strides=(1, 1), padding='valid', name='pool2')(conv2)\n",
    "        \n",
    "        concatenated = Concatenate(axis = 1)([pooled0, pooled1, pooled2])\n",
    "        flattened = keras.layers.Flatten()(concatenated)\n",
    "        dropped1 = Dropout(drop_rate, name='drop1')(flattened)  \n",
    "        prob = Dense(self.nbCategories, activation='softmax', name='dense2')(dropped1)\n",
    "\n",
    "        self.model = Model(my_input, prob)\n",
    "\n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model variables\n",
    "TRAINABLE = False # never train the embedding for the classification task (overfitting)\n",
    "PRE_TRAINED_DIM = 300 # Size of the pretrained embedding used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instance creation\n",
    "model = CustomModel(nbCategories=NB_CATEGORIES, trainable=TRAINABLE, medicaments=MEDICAMENTS)\n",
    "\n",
    "# Loading, parsing and spliting training and testing data\n",
    "x = pd.read_csv(xPath, delimiter=';', usecols=[1]).values.ravel()\n",
    "y = pd.read_csv(yPath, delimiter=';', usecols=[1]).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct spelling mistakes\n",
    "corrected_dict = {}\n",
    "for key, val in csv.reader(open(correctionsPath)):\n",
    "    corrected_dict[key] = val\n",
    "for i, s in enumerate(x):\n",
    "    x[i] = model.spellingCorrection(s, corrected_dict, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  10141\n"
     ]
    }
   ],
   "source": [
    "# Print some info about our vocabulary\n",
    "model.preprocess(x)\n",
    "x_vocab  = list(model.tokenizer.word_index.keys())\n",
    "print('Vocabulary size: ', len(x_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Embedding Created ----------\n",
      "Number of words in corpus that do not appear in pretrained Fasttext:  2243\n"
     ]
    }
   ],
   "source": [
    "# Loading and using pretrained embedding\n",
    "\n",
    "# Using fasttext\n",
    "#path2embedding = '../word_embeddings/wiki.fr.vec'\n",
    "# using fasttext trained on wiki_fr and emea database\n",
    "path2embedding = '../word_embeddings/retrained_fr.vec'\n",
    "pre_trained_wv = gensim.models.KeyedVectors.load_word2vec_format(path2embedding,\n",
    "                                                                 binary=False)\n",
    "\n",
    "# We use an embedding size of len(x_vocab) + 1 because the 0 is used for the padding\n",
    "embeddings = np.zeros((len(x_vocab) + 1 , PRE_TRAINED_DIM))\n",
    "not_in_pretrained = []\n",
    "\n",
    "for word, idx in model.tokenizer.word_index.items():\n",
    "    if word not in pre_trained_wv.vocab:\n",
    "        vec = np.zeros(PRE_TRAINED_DIM)\n",
    "        not_in_pretrained.append(word)\n",
    "    else:\n",
    "        vec = pre_trained_wv[word]\n",
    "\n",
    "    # word_to_index is 1-based! the 0-th row, used for padding, stays at zero\n",
    "    embeddings[idx,] = vec\n",
    "\n",
    "print('---------- Embedding Created ----------')\n",
    "print('Number of words in corpus that do not appear in '\n",
    "      'pretrained Fasttext: ', len(not_in_pretrained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the embedding for convenience\n",
    "#np.save('challenge_data/fasttext_voc_not_corrected.npy', embeddings)\n",
    "#np.save('challenge_data/fasttext_emb.npy', embeddings)\n",
    "np.save('challenge_data/fasttext_retrained.npy', embeddings)\n",
    "#embeddings = np.load('challenge_data/fasttext_emb.npy')\n",
    "#embeddings = np.load('challenge_data/fasttext_voc_not_corrected.npy')\n",
    "embeddings = np.load('challenge_data/fasttext_retrained.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 150, 300)          3042600   \n",
      "_________________________________________________________________\n",
      "drop0 (Dropout)              (None, 150, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 150, 256)          230656    \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling1D)         (None, 75, 256)           0         \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 75, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 52)                5252      \n",
      "=================================================================\n",
      "Total params: 3,421,308\n",
      "Trainable params: 378,708\n",
      "Non-trainable params: 3,042,600\n",
      "_________________________________________________________________\n",
      "Total number of model parameters: 3421308\n"
     ]
    }
   ],
   "source": [
    "# Model parameters among (drop_rate=0.3, nb_filters=32, filter_size=3)\n",
    "DROP_RATE = 0.3\n",
    "NB_FILTERS = 256\n",
    "FILTER_SIZE = 3\n",
    "#FILTERS_SIZES = []\n",
    "# Build our model\n",
    "model.buildLSTM_CNN(embeddings, drop_rate=DROP_RATE, nb_filters=NB_FILTERS,\n",
    "                    filter_size=FILTER_SIZE)\n",
    "model.model.summary()\n",
    "\n",
    "print('Total number of model parameters:', model.model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input data and labels before training\n",
    "y = model.preprocessLabels(y)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xTrain = model.preprocess(xTrain)\n",
    "xTest = model.preprocess(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6422 samples, validate on 1606 samples\n",
      "Epoch 1/10\n",
      "6422/6422 [==============================] - 41s 6ms/step - loss: 3.1928 - acc: 0.2301 - val_loss: 2.9828 - val_acc: 0.2771\n",
      "Epoch 2/10\n",
      "6422/6422 [==============================] - 24s 4ms/step - loss: 2.9359 - acc: 0.2778 - val_loss: 2.8483 - val_acc: 0.3107\n",
      "Epoch 3/10\n",
      "6422/6422 [==============================] - 26s 4ms/step - loss: 2.7437 - acc: 0.3197 - val_loss: 2.7539 - val_acc: 0.3163\n",
      "Epoch 4/10\n",
      "6422/6422 [==============================] - 28s 4ms/step - loss: 2.5780 - acc: 0.3539 - val_loss: 2.6977 - val_acc: 0.3294\n",
      "Epoch 5/10\n",
      "6422/6422 [==============================] - 28s 4ms/step - loss: 2.4261 - acc: 0.3832 - val_loss: 2.6657 - val_acc: 0.3344\n",
      "Epoch 6/10\n",
      "6422/6422 [==============================] - 30s 5ms/step - loss: 2.2642 - acc: 0.4223 - val_loss: 2.6866 - val_acc: 0.3319\n",
      "Epoch 7/10\n",
      "6422/6422 [==============================] - 30s 5ms/step - loss: 2.1172 - acc: 0.4604 - val_loss: 2.7865 - val_acc: 0.2970\n",
      "Epoch 8/10\n",
      "6422/6422 [==============================] - 31s 5ms/step - loss: 2.0422 - acc: 0.4723 - val_loss: 2.8604 - val_acc: 0.2808\n",
      "Epoch 9/10\n",
      "6422/6422 [==============================] - 30s 5ms/step - loss: 1.9527 - acc: 0.4908 - val_loss: 2.7028 - val_acc: 0.3412\n",
      "Epoch 10/10\n",
      "6422/6422 [==============================] - 31s 5ms/step - loss: 1.8708 - acc: 0.5087 - val_loss: 2.7830 - val_acc: 0.3306\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "model.train(xTrain, yTrain, epochs=EPOCHS, batch_size= BATCH_SIZE,\n",
    "            validation_data=(xTest, yTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606/1606 [==============================] - 2s 1ms/step\n",
      "Accuracy: 33.06 %\n",
      "Loss: [2.6934158825785257, 0.33063511830635117]\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(xTest, yTest)\n",
    "\n",
    "prediction = model.predict(xTest)\n",
    "predictionCategories = np.argmax(prediction, axis=1)\n",
    "yTestCategories = np.argmax(yTest, axis=1)\n",
    "accuracy = 100 * sum([predictionCategories[i] == yTestCategories[i]\n",
    "                      for i in range(len(yTestCategories))]) / len(yTestCategories)\n",
    "\n",
    "print('Accuracy: {:.2f} %\\nLoss: {}'.format(accuracy, str(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAEyCAYAAACLaSO4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRNwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad66937e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.hist(y, bins=NB_CATEGORIES, label='train labels', density=True, alpha=1)\n",
    "plt.hist(predictionCategories, bins=NB_CATEGORIES,\n",
    "         label='predicted labels', density=True, alpha=0.6)\n",
    "plt.axis\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37, 44, 44, ..., 48, 45, 44])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTestCategories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
