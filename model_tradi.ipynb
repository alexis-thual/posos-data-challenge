{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model based on similarity of keys words\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = ['alors','au','aucuns','aussi','autre','avant','avec','avoir','bon','car',\n",
    "              'ce','cela','ces','ceux','chaque','ci','comme','comment','dans','des',\n",
    "              'du','dedans','dehors','depuis','devrait','doit','donc','dos','début','elle',\n",
    "              'elles','en','encore','essai','est','et','eu','fait','faites','fois',\n",
    "              'font','hors','ici','il','ils','je','juste','la','le','les',\n",
    "              'leur','là','ma','maintenant','mais','mes','mine','moins','mon','mot',\n",
    "              'même','ni','nommés','notre','nous','ou','où','par','parce','pas',\n",
    "              'peut','peu','plupart','pour','pourquoi','quand','que','quel','quelle','quelles',\n",
    "              'quels','qui','sa','sans','ses','seulement','si','sien','son','sont',\n",
    "              'sous','soyez','sujet','sur','ta','tandis','tellement','tels','tes','ton',\n",
    "              'tous','tout','trop','très','tu','voient','vont','votre','vous','vu',\n",
    "              'ça','étaient','état','étions','été','être', 'de', 'un', 'une', 'ai', 'ne', 'on']\n",
    "\n",
    "def vectorizeVocabulary(corpus, verbose=False, density=False):\n",
    "    # Generate word tokens\n",
    "    countVectorizer = CountVectorizer(input='content')\n",
    "    countVector = countVectorizer.fit_transform(corpus)\n",
    "    vocabulary = countVectorizer.vocabulary_\n",
    "    wordCount = np.sum(countVector, axis=0)\n",
    "    totalWordCount = np.sum(wordCount)\n",
    "\n",
    "    vocabulary = list(map(partial(parseWord, wordCount,\n",
    "                                  totalWordCount, density), vocabulary.items()))\n",
    "    \n",
    "    # Sort words by usage\n",
    "    sortedVocabulary = sorted(vocabulary, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"countVector.shape: {}\".format(str(countVector.shape)))\n",
    "        print(\"wordCount.shape: {}\".format(str(wordCount.shape)))\n",
    "        print(sortedVocabulary[:5])\n",
    "    \n",
    "    return sortedVocabulary\n",
    "\n",
    "def selectCat(x, y, cat_index):\n",
    "    cat_ix = []\n",
    "    for xx, yy in zip(x, y):\n",
    "        if yy[1] == cat_index:\n",
    "            cat_ix.append(xx[0])\n",
    "    cat_ix = np.array(cat_ix)\n",
    "    selected_questions = x[cat_ix]\n",
    "    return selected_questions\n",
    "\n",
    "def extract_top_k(x, y, k, stop_words = STOP_WORDS):\n",
    "    best_vocab = []\n",
    "    nb_cat = len(set(y[:,1]))\n",
    "    for cat in np.arange(nb_cat):\n",
    "        questions = selectCat(x, y, cat)[0][:,1]\n",
    "        cat_vocab = vectorizeVocabulary(corpus=questions)\n",
    "        cat_best_vocab = []\n",
    "        for w in cat_vocab:\n",
    "            if len(cat_best_vocab) < k:\n",
    "                if w[0] not in stop_words:\n",
    "                    cat_best_vocab.append(w)           \n",
    "        best_vocab.append(cat_best_vocab)\n",
    "    return best_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel():\n",
    "    '''Generic workflow class.'''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.verbose = kwargs.get('verbose', False)\n",
    "\n",
    "        self.nbCategories = kwargs['nbCategories']\n",
    "        self.paddingLength = PADDING\n",
    "        self.maxNumberWords = (1e5)\n",
    "        self.trainable = kwargs.get('trainable', False)\n",
    "\n",
    "        self.tokenizer = text.Tokenizer(\n",
    "            num_words=self.maxNumberWords,\n",
    "            filters=\"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'´’™©®«»\",\n",
    "            split=\" \"\n",
    "        )\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        '''Turns sentences into padded word sequences.'''\n",
    "\n",
    "        self.tokenizer.fit_on_texts(x)\n",
    "        sequences = self.tokenizer.texts_to_sequences(x)\n",
    "        sequences = sequence.pad_sequences(sequences, self.paddingLength)\n",
    "\n",
    "        return sequences\n",
    "\n",
    "    def preprocessLabels(self, labels):\n",
    "        return to_categorical(labels, num_classes=self.nbCategories)\n",
    "    \n",
    "    # return a list of 51 sublists each containing k tuples (keyword, nb_occurences)\n",
    "    def extract_top_k(self, x, y, k, stop_words = STOP_WORDS):\n",
    "        best_vocab = []\n",
    "        nb_cat = len(set(y[:,1]))\n",
    "        for cat in np.arange(nb_cat):\n",
    "            questions = selectCat(x, y, cat)[0][:,1]\n",
    "            cat_vocab = vectorizeVocabulary(corpus=questions)\n",
    "            cat_best_vocab = []\n",
    "            for w in cat_vocab:\n",
    "                if len(cat_best_vocab) < k:\n",
    "                    if w[0] not in stop_words:\n",
    "                        cat_best_vocab.append(w)           \n",
    "            best_vocab.append(cat_best_vocab)\n",
    "            \n",
    "            self.best_vocab = best_vocab\n",
    "        return best_vocab\n",
    "\n",
    "    def train(self, x, y, epochs= 10, batch_size=32, validation_data=None):\n",
    "        if callback == True:\n",
    "            filepath= 'models_checkpoints/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc',\n",
    "                                         verbose=1, save_best_only=True, mode='max')\n",
    "            callbacks_list = [checkpoint]\n",
    "\n",
    "            self.model.fit(x, y, shuffle='batch', epochs=epochs,\n",
    "                           batch_size=batch_size, validation_data=validation_data,\n",
    "                           callbacks=callbacks_list)\n",
    "        else:\n",
    "            self.model.fit(x, y, shuffle='batch', epochs=epochs,\n",
    "                           batch_size=batch_size, validation_data=validation_data)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        return self.model.evaluate(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model.predict(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
